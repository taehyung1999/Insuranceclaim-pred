{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=pd.read_csv(\"instrain.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the data heads\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can see that the missing values are represent by -1 so we convert them with NaN\n",
    "data1=data1.replace(to_replace=-1,value = np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Metadata DataFrame so that it will be easy to handle and manipulate data during data exploration steps\n",
    "\n",
    "# role: input, ID, target\n",
    "# level: nominal, interval, ordinal, binary\n",
    "# keep: True or False\n",
    "# dtype: int, float, str\n",
    "\n",
    "def create_metadata_dataframe(data1):\n",
    "    # Define an empty DataFrame with specified columns\n",
    "    MetaData = pd.DataFrame(columns=['variable', 'role', 'level', 'keep', 'dtype', 'unique_values'])\n",
    "    data = []\n",
    "\n",
    "    for column in data1.columns:  # It iterates over each column\n",
    "        missing_values = data1[column].isnull().sum()\n",
    "        unique_values = data1[column].nunique()\n",
    "\n",
    "        # Defining Role\n",
    "        if column == 'id':\n",
    "            role = 'id'\n",
    "        elif column == 'target':\n",
    "            role = 'target'\n",
    "        else:\n",
    "            role = 'input'\n",
    "\n",
    "        # Defining DataTypes\n",
    "        dtype = data1[column].dtype\n",
    "\n",
    "        # Defining keep\n",
    "        keep = True\n",
    "\n",
    "        # Defining Level\n",
    "        if column[-3:] == 'bin' or column == 'target':\n",
    "            level = 'binary'\n",
    "        elif column[-3:] == 'cat' or column == 'id':\n",
    "            level = 'categorical'\n",
    "        elif data1[column].dtype == float:\n",
    "            level = 'interval'\n",
    "        elif data1[column].dtype == np.int64:\n",
    "            level = 'ordinal'\n",
    "\n",
    "        # Create a dictionary with metadata for the current column\n",
    "        f_dict = {\n",
    "            'variable': column,\n",
    "            'role': role,\n",
    "            'level': level,\n",
    "            'keep': keep,\n",
    "            'dtype': dtype,\n",
    "            'unique_values': unique_values,\n",
    "        }\n",
    "\n",
    "        # Append the dictionary to the list\n",
    "        data.append(f_dict)\n",
    "\n",
    "    # Append the list of dictionaries to the metadata DataFrame\n",
    "    MetaData = MetaData.append(data, ignore_index=True)\n",
    "\n",
    "    # Return the final metadata DataFrame\n",
    "    return MetaData\n",
    "\n",
    "\n",
    "# Read your dataset\n",
    "your_dataset_path = 'instrain.csv'\n",
    "your_dataframe = pd.read_csv(your_dataset_path)\n",
    "\n",
    "# Call the function to create metadata\n",
    "metadata_dataframe = create_metadata_dataframe(your_dataframe)\n",
    "\n",
    "# Print the metadata DataFrame\n",
    "print(metadata_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining numerical and categorical columns\n",
    "numeric_features = [feature for feature in data1.columns if data1[feature].dtype != 'O']\n",
    "categorical_features=[feature for feature in data1.columns if data1[feature].dtype == 'O']\n",
    "\n",
    "#print columns \n",
    "print('we have {} numeric features : {}'.format(len(numeric_features),numeric_features))\n",
    "print('\\nwe have categorical features :{}'.format(len(categorical_features), categorical_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for missing values\n",
    "data1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through this we can see that features have a lot of missing values FEATURE ps_car_03_cat 411231 ps_reg_03 107772 ps_car_05_cat 266551 ps_car_14 42620 ps_car_07_cat 11489\n",
    "\n",
    "Droping all the features with highest missing values For rest of the missing values we will be replacing it with mean and mode binary data is represented by bin(mode) categorical data is represented by cat(mode) rest are the cordinal and ordinal data(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.drop([\"ps_car_03_cat\",\"ps_reg_03\",\"ps_car_05_cat\",\"ps_car_14\",\"ps_car_07_cat\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing missing data in the remaining features\n",
    "feature_bin = [f for f in data1.columns if f.endswith('bin')] \n",
    "feature_cat = [f for f in data1.columns if f.endswith('cat')] \n",
    "feature_els = [f for f in data1.columns if (f not in feature_bin) & (f not in feature_cat) & (f not in ['id', 'target'])]\n",
    "\n",
    "for f in (feature_bin + feature_cat):\n",
    "    data1[f].fillna(value=data1[f].mode()[0], inplace=True)\n",
    "for f in feature_els:\n",
    "    data1[f].fillna(value=data1[f].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HEATMAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.title('Pearson correlation of bin features', y=1.05, size=15)\n",
    "sns.heatmap(data1[feature_bin].corr(),\n",
    "            linewidths=0.1,\n",
    "            vmax=1.0, \n",
    "            square=True, \n",
    "            linecolor='white', \n",
    "            annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Pearson correlation of cat features', y=1.05, size=15)\n",
    "sns.heatmap(data1[feature_cat].corr(),\n",
    "            linewidths=0.1,\n",
    "            vmax=1.0, \n",
    "            square=True, \n",
    "            linecolor='white', \n",
    "            annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "correlations between all numeric variables in your dataset. Values range from -1 to 1, where:\n",
    "\n",
    "1 indicates a perfect positive correlation,\n",
    "-1 indicates a perfect negative correlation, and\n",
    "0 indicates no correlation.\n",
    "\n",
    "\"The Pearson correlation coefficient between column1 and column2 is 0.75, indicating a strong positive linear relationship.\"\n",
    "highly correlated features can sometimes lead to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting Categorical Data into numerical data using One Hot Encoding One hot encoding is a technique that we use to represent categorical variables as numerical values in a machine learning model. LabelEncoder assigns a numerical label to each category, while OneHotEncoder creates a binary vector representation of the categorical data, where each column represents a unique category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding for categorical variables\n",
    "\n",
    "features_cat = [c for c in data1 if 'cat' in c]\n",
    "for f in features_cat:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(data1[f])\n",
    "    data1[f] = le.transform(data1[f])\n",
    "\n",
    "\n",
    "# Label encoding for categorical variables\n",
    "\n",
    "print('Before encoding we have {} variables in data1'.format(data1.shape[1]))\n",
    "print('After encoding we have {} variables in data1'.format(data1.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing low variance variables from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Create selector to remove low-variance features\n",
    "selector = VarianceThreshold(threshold=.01)\n",
    "\n",
    "# Fit selector to train data without 'id' and 'target' columns\n",
    "selector.fit(data1.drop(['id', 'target'], axis=1))\n",
    "\n",
    "# Identify low-variance features\n",
    "low_variance_cols = data1.drop(['id', 'target'], axis=1).columns[selector.variances_ < .01]\n",
    "\n",
    "# Print number and names of low-variance features\n",
    "print('{} variables have too low variance.'.format(len(low_variance_cols)))\n",
    "print('These variables are {}'.format(list(low_variance_cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the low variance variables from the data and checking the shape of the final dataframe\n",
    "\n",
    "data2 = data1.drop(list(low_variance_cols), axis = 1)\n",
    "data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL CREATION\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(data2.drop(['target', 'id'], axis=1),data2['target'].astype(int), test_size=0.30, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_temp = X_train\n",
    "df_train_temp['target'] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minority = df_train_temp[df_train_temp.target==1]\n",
    "df_minority.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Splitting the data into majority and minority classes\n",
    "df_majority = df_train_temp[df_train_temp.target == 0]\n",
    "df_minority = df_train_temp[df_train_temp.target == 1]\n",
    "\n",
    "# Determine size of minority class\n",
    "minority_class_size = df_minority.shape[0]\n",
    "\n",
    "# Downsample majority class to match minority class\n",
    "df_majority_downsampled = resample(df_majority, replace=False, n_samples=minority_class_size, random_state=123)\n",
    "\n",
    "# Combine majority class with minority class\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "# Display the class distribution of the undersampled data\n",
    "df_downsampled.target.value_counts()\n",
    "\n",
    "# Upsample minority class to match majority class\n",
    "df_majority = df_downsampled[df_downsampled.target == 0]\n",
    "df_minority = df_downsampled[df_downsampled.target == 1]\n",
    "\n",
    "# Determine size of majority class\n",
    "majority_class_size = df_majority.shape[0]\n",
    "\n",
    "# Upsample minority class to match majority class\n",
    "df_minority_upsampled = resample(df_minority, replace=True, n_samples=majority_class_size, random_state=123)\n",
    "\n",
    "# Combine majority class with upsampled minority class\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "# Display the new class distribution\n",
    "df_upsampled.target.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##the output of upsampling results in a higher number of positive examples (target=1), which may be beneficial if we want to improve the model's ability to predict positive examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_up = df_upsampled.drop('target', axis = 1)\n",
    "y_train_up = df_upsampled.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE SCALING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mms = StandardScaler()\n",
    "X_train_up_scaled = mms.fit_transform(X_train_up)\n",
    "#y_train_up_scaled = mms.fit_transform(y_train_up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MACHINE LEARNING MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logistic_Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Fit logistic regression model on upsampled data\n",
    "lm = LogisticRegression()\n",
    "logistic_model = lm.fit(X_train_up, y_train_up)\n",
    "\n",
    "# Evaluate model on test data\n",
    "predictions_LM = logistic_model.predict(X_test)\n",
    "\n",
    "print('Classification Report - Logistic Regression')\n",
    "print(classification_report(y_test, predictions_LM))\n",
    "\n",
    "print('Confusion Matrix - Logistic Regression')\n",
    "print(confusion_matrix(y_test, predictions_LM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision: It is the ratio of true positive predictions to the total number of predicted positive instances. For the \"0\" class, the precision is 0.97 which means that 97% of the instances predicted as \"0\" are actually \"0\". For the \"1\" class, the precision is 0.05 which means that only 5% of the instances predicted as \"1\" are actually \"1\". Recall: It is the ratio of true positive predictions to the total number of actual positive instances. For the \"0\" class, the recall is 0.62 which means that 62% of the actual \"0\" instances are correctly identified by the model. For the \"1\" class, the recall is 0.55 which means that 55% of the actual \"1\" instances are correctly identified by the model. F1-score: It is the harmonic mean of precision and recall. It provides a balance between precision and recall. For the \"0\" class, the F1-score is 0.76 which is a weighted average of precision and recall. For the \"1\" class, the F1-score is 0.09 which indicates poor performance. Accuracy: It is the ratio of the total number of correct predictions to the total number of predictions. The overall accuracy of the model is 0.62 which means that the model correctly predicted 62% of the instances in the test set. Support: It is the number of instances in each class. The confusion matrix shows the actual and predicted class labels. The rows represent the actual classes, and the columns represent the predicted classes. The elements of the confusion matrix are as follows:\n",
    "\n",
    "True Negative (TN): The number of instances that are actually negative and predicted as negative. In this case, there are 106,297 TN instances of the \"0\" class. False Positive (FP): The number of instances that are actually negative but predicted as positive. In this case, there are 65,754 FP instances of the \"1\" class. False Negative (FN): The number of instances that are actually positive but predicted as negative. In this case, there are 2,913 FN instances of the \"0\" class. True Positive (TP): The number of instances that are actually positive and predicted as positive. In this case, there are 3,600 TP instances of the \"1\" class. The confusion matrix shows that the model is correctly predicting most of the instances of the \"0\" class but poorly predicting instances of the \"1\" class. This could be due to the class imbalance issue in the dataset. The model may need to be trained on a more balanced dataset or use techniques like resampling, regularization, or different classification algorithms to improve performance on the minority class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KNeighborsClassifier from sklearn.neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Instantiate the KNeighborsClassifier object\n",
    "clf_KNN = KNeighborsClassifier(n_neighbors=2)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "clf_KNN.fit(X_train_up, y_train_up)\n",
    "\n",
    "# Make predictions using the trained classifier on the test data\n",
    "predictions_KNN = clf_KNN.predict(X_test)\n",
    "\n",
    "# Print the classification report for the predictions\n",
    "print('Classification Report - Nearest Neighbors')\n",
    "print(classification_report(y_test, predictions_KNN))\n",
    "\n",
    "# Print the confusion matrix for the predictions\n",
    "print('Confusion Matrix - Nearest Neighbors')\n",
    "print(confusion_matrix(y_test, predictions_KNN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Nearest Neighbors model, the precision for class 0 is 0.96, which means that 96% of the samples predicted as class 0 were actually class 0. The recall for class 0 is 0.77, which means that 77% of the actual class 0 samples were correctly predicted as class 0. Similarly, the precision and recall for class 1 are 0.04 and 0.25, respectively, which indicates poor performance for this class.\n",
    "\n",
    "The confusion matrix shows the actual and predicted class labels for the test set. The rows correspond to the actual class labels, and the columns correspond to the predicted class labels. In this case, the model correctly predicted 132149 samples as class 0, but incorrectly predicted 39902 samples as class 1. Similarly, the model correctly predicted 1632 samples as class 1, but incorrectly predicted 4881 samples as class 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randominzed logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Assuming X_train_up and y_train_up are your upsampled training data\n",
    "# and X_test, y_test are your test data\n",
    "\n",
    "logreg = LogisticRegression(class_weight='balanced')\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "param = {'C': [0.001, 0.003, 0.005, 0.01, 0.03, 0.05, 0.1, 0.3, 0.5, 1]}\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "clf_RLR = RandomizedSearchCV(logreg, param_distributions=param, scoring='roc_auc', refit=True, cv=3)\n",
    "\n",
    "# Perform the randomized search on the upsampled training data\n",
    "clf_RLR.fit(X_train_up, y_train_up)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions_RLR = clf_RLR.predict(X_test)\n",
    "\n",
    "# Print confusion matrix and classification report\n",
    "print('Confusion Matrix RandomizedSearchCV:')\n",
    "print(confusion_matrix(y_test, predictions_RLR))\n",
    "\n",
    "print('\\nClassification Report RandomizedSearchCV:')\n",
    "print(classification_report(y_test, predictions_RLR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the RandomizedSearchCV model, the precision for class 0 is 0.97, which means that 97% of the samples predicted as class 0 were actually class 0. The recall for class 0 is 0.62, which means that 62% of the actual class 0 samples were correctly predicted as class 0. Similarly, the precision and recall for class 1 are 0.05 and 0.55, respectively, which indicates poor performance for this class.\n",
    "\n",
    "The confusion matrix shows the actual and predicted class labels for the test set. The rows correspond to the actual class labels, and the columns correspond to the predicted class labels. In this case, the model correctly predicted 106124 samples as class 0, but incorrectly predicted 65927 samples as class 1. Similarly, the model correctly predicted 3602 samples as class 1, but incorrectly predicted 2911 samples as class 0.\n",
    "\n",
    "Overall, the model seems to have performed poorly, with a low accuracy and F1-score. The recall for the positive class (class 1) is relatively high compared to the precision, which indicates that the model is better at identifying positive samples but is less precise in doing so. This could indicate a class imbalance in the dataset or a need for more fine-tuning of the model hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "# Train an SVM classifier with a radial basis function (RBF) kernel\n",
    "svm = SVC(kernel='rbf', random_state=42)\n",
    "svm.fit(X_train_up, y_train_up)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the classifier\n",
    "print(\"Classification Report - SVM\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix - SVM\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision for class 0 is 0.97, which means that out of all the predicted negatives (0s), 97% of them were true negatives. The recall for class 0 is 0.66, which means that out of all the actual negatives (0s), 66% of them were correctly identified by the model. The F1-score for class 0 is 0.79, which is the harmonic mean of precision and recall.\n",
    "\n",
    "The precision for class 1 is very low at 0.05, which means that out of all the predicted positives (1s), only 5% of them were true positives. The recall for class 1 is 0.49, which means that out of all the actual positives (1s), only 49% of them were correctly identified by the model. The F1-score for class 1 is 0.09, which is quite low.\n",
    "\n",
    "The confusion matrix shows that the model correctly predicted 113,748 true negatives (0s) and 3,210 true positives (1s), but also incorrectly predicted 58,303 false negatives (0s) and 3,303 false positives (1s).\n",
    "\n",
    "Overall, the model's performance is better than the logistic regression and nearest neighbors models, but still not ideal, especially for predicting the positive class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Instantiate the model with default hyperparameters\n",
    "gb_cllf = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "gb_cllf.fit(X_train_up, y_train_up)\n",
    "\n",
    "# Predict the target variable for the test data\n",
    "y_pred = gb_cllf.predict(X_test)\n",
    "\n",
    "# Evaluate the model using classification report and confusion matrix\n",
    "print(\"Classification Report - Gradient Boosting\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix - Gradient Boosting\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of Gradient Boosting seem to be similar to Logistic Regression and RandomizedSearchCV models. The precision, recall, and F1-score for the positive class (fraudulent transactions) are low, indicating that the model is not performing well in detecting frauds. However, the accuracy is high, which means that the model is correctly predicting most of the non-fraudulent transactions.\n",
    "\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decision tree classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 2000, max_depth=6,min_samples_split=70,min_samples_leaf=30)\n",
    "classifier.fit(X_train_up, y_train_up)\n",
    "\n",
    "# Predict the target variable for the test data\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model using classification report and confusion matrix\n",
    "print(\"Classification Report - Decision tree\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print('RF Score: ', metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix - Decision tree\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# View accuracy score\n",
    "accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Instantiate a decision tree classifier\n",
    "dtc = DecisionTreeClassifier(random_state=123)\n",
    "\n",
    "# Instantiate a bagging classifier\n",
    "bagging = BaggingClassifier(dtc, n_estimators=100, random_state=123)\n",
    "\n",
    "# Train the bagging classifier\n",
    "bagging.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = bagging.predict(X_test)\n",
    "\n",
    "# Calculate the test set accuracy\n",
    "test_score = bagging.score(X_test, y_test)\n",
    "\n",
    "# Print the test set accuracy\n",
    "print(\"Bagging Test Set Score:\", test_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Instantiate a decision tree classifier\n",
    "dtc = DecisionTreeClassifier(random_state=123)\n",
    "\n",
    "# Instantiate an AdaBoost classifier\n",
    "adaboost = AdaBoostClassifier(dtc, n_estimators=100, random_state=123)\n",
    "\n",
    "# Train the AdaBoost classifier\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = adaboost.predict(X_test)\n",
    "\n",
    "# Calculate the test set accuracy\n",
    "test_score = adaboost.score(X_test, y_test)\n",
    "\n",
    "# Print the test set accuracy\n",
    "print(\"Boosting Test Set Score:\", test_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_upsampled.drop('target', axis=1), df_upsampled.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Upsample minority class to match majority class\n",
    "df_majority = df_upsampled[df_upsampled.target == 0]\n",
    "df_minority = df_upsampled[df_upsampled.target == 1]\n",
    "\n",
    "# Determine size of majority class\n",
    "majority_class_size = df_majority.shape[0]\n",
    "\n",
    "# Upsample minority class to match majority class\n",
    "df_minority_upsampled = resample(df_minority, replace=True, n_samples=majority_class_size, random_state=123)\n",
    "\n",
    "# Combine majority class with upsampled minority class\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "# Separate features and target variable\n",
    "X_train = df_upsampled.drop('target', axis=1)\n",
    "y_train = df_upsampled['target']\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "lr_model = LogisticRegression()\n",
    "\n",
    "# Train Logistic Regression model\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Initialize K-Nearest Neighbors model\n",
    "knn_model = KNeighborsClassifier()\n",
    "\n",
    "# Train K-Nearest Neighbors model\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Initialize Gradient Boosting model\n",
    "gb_model = GradientBoostingClassifier()\n",
    "\n",
    "# Train Gradient Boosting model\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Instantiate a bagging classifier\n",
    "bagging = BaggingClassifier()\n",
    "\n",
    "# Train the bagging classifier\n",
    "bagging.fit(X_train, y_train)\n",
    "\n",
    "# Instantiate an AdaBoost classifier\n",
    "adaboost = AdaBoostClassifier()\n",
    "\n",
    "# Train the AdaBoost classifier\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Initialize Voting Classifier with Logistic Regression, K-Nearest Neighbors, and Gradient Boosting models\n",
    "voting_model = VotingClassifier(estimators=[('lr', lr_model), ('knn', knn_model), ('gb', gb_model)], voting='hard')\n",
    "\n",
    "# Train Voting Classifier\n",
    "voting_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the performance of each model and the Voting Classifier on the test set\n",
    "print('Logistic Regression Test Set Score:', lr_model.score(X_test, y_test))\n",
    "print('K-Nearest Neighbors Test Set Score:', knn_model.score(X_test, y_test))\n",
    "print('Gradient Boosting Test Set Score:', gb_model.score(X_test, y_test))\n",
    "print('Voting Classifier Test Set Score:', voting_model.score(X_test, y_test))\n",
    "print(\"Bagging Test Set Score:\", bagging.score(X_test, y_test))\n",
    "print(\"Boosting Test Set Score:\",adaboost.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1:-Create a predictive model which will help the insurance marketing team to know which customer will buy the product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, the developed predictive model, particularly the Randomized Logistic Regression and the ensemble Voting Classifier, can be valuable tools for the insurance marketing team. These models offer insights into potential customers likely to purchase the insurance product, aiding the team in targeted marketing strategies and resource allocation for more effective and efficient campaigns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Task2 : Suggestions to the Insurance market team to make  customers  buy the product.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear Communication: Clearly explain insurance product details.\n",
    "\n",
    "Educational Content: Provide simple, informative content.\n",
    "\n",
    "Targeted Marketing: Personalize campaigns for specific customer groups.\n",
    "\n",
    "Discounts and Promotions: Offer incentives and limited-time offers.\n",
    "\n",
    "Online Presence: Ensure a user-friendly website and app.\n",
    "\n",
    "Customer Testimonials: Share positive customer experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report on Challenges faced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report on Data Analysis Challenges and Techniques Used\n",
    "\n",
    "1. Data Exploration and Preprocessing:\n",
    "\n",
    "Challenge: Missing Values represented by -1.\n",
    "Technique Used: Replaced missing values with NaN.\n",
    "Reason: NaN values are easier to handle in subsequent analyses, imputations, and visualizations.\n",
    "2. Metadata Creation:\n",
    "\n",
    "Challenge: Managing the metadata information for each variable.\n",
    "Technique Used: Created a metadata DataFrame to categorize variables based on role, level, and data type.\n",
    "Reason: Simplifies data exploration, feature engineering, and model building by providing organized metadata.\n",
    "3. Feature Selection:\n",
    "\n",
    "Challenge: Dealing with numerous features and identifying irrelevant ones.\n",
    "Technique Used: Removed low-variance features using VarianceThreshold.\n",
    "Reason: Reduces dimensionality by eliminating features with low variance, improving computational efficiency.\n",
    "4. Handling Missing Values:\n",
    "\n",
    "Challenge: Features with high missing values, e.g., ps_car_03_cat, ps_reg_03, etc.\n",
    "Technique Used: Dropped features with the highest missing values and imputed the rest using mean or mode.\n",
    "Reason: Focus on relevant features and prevent loss of valuable information.\n",
    "5. Categorical Data Encoding:\n",
    "\n",
    "Challenge: Converting categorical variables into a numerical format.\n",
    "Technique Used: Applied one-hot encoding for categorical variables.\n",
    "Reason: Enables machine learning models to process categorical data effectively.\n",
    "6. Model Training and Class Imbalance:\n",
    "\n",
    "Challenge: Class imbalance after upsampling.\n",
    "Technique Used: Utilized ensemble models (Voting Classifier) and evaluated their performance.\n",
    "Reason: Combining multiple models can address the impact of class imbalance and enhance overall prediction accuracy.\n",
    "7. Hyperparameter Tuning:\n",
    "\n",
    "Challenge: Optimizing hyperparameters for Logistic Regression using RandomizedSearchCV.\n",
    "Technique Used: Implemented RandomizedSearchCV for hyperparameter tuning.\n",
    "Reason: Efficiently explores hyperparameter space, improving model performance.\n",
    "8. Model Evaluation and Interpretation:\n",
    "\n",
    "Challenge: Assessing model performance comprehensively.\n",
    "Technique Used: Utilized classification metrics (Precision, Recall, F1-score) and confusion matrices.\n",
    "Reason: Provides insights into model strengths, weaknesses, and trade-offs between false positives and false negatives.\n",
    "9. Machine Learning Models:\n",
    "\n",
    "Challenges: Poor performance, especially in predicting the positive class.\n",
    "Techniques Used: Explored Logistic Regression, K-Nearest Neighbors, Gradient Boosting, SVM, Decision Tree, Bagging, and AdaBoost.\n",
    "Reason: Comparative analysis to identify the most suitable model based on the dataset characteristics.\n",
    "10. Ensemble Learning:\n",
    "\n",
    "Challenge: Improving model robustness.\n",
    "Technique Used: Employed Voting Classifier with Logistic Regression, K-Nearest Neighbors, and Gradient Boosting.\n",
    "Reason: Combining diverse models can lead to better overall performance by leveraging their individual strengths.\n",
    "11. Feature Scaling:\n",
    "\n",
    "Challenge: Ensuring consistency in feature scales.\n",
    "Technique Used: Applied StandardScaler for feature scaling.\n",
    "Reason: Standardized features to a common scale, preventing certain features from dominating the modeling process.\n",
    "12. Report and Conclusion:\n",
    "\n",
    "Challenge: Summarizing findings and recommendations.\n",
    "Technique Used: Created a comprehensive report summarizing challenges, techniques, and model evaluations.\n",
    "Reason: Provides a clear overview for stakeholders, facilitating decision-making and future improvements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a report stating the performance of multiple models on this data and suggest the best model for production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression:\n",
    "\n",
    "Precision (Class 1): 0.05\n",
    "Recall (Class 1): 0.55\n",
    "F1-score (Class 1): 0.09\n",
    "Accuracy: 0.62\n",
    "K-Nearest Neighbors:\n",
    "\n",
    "Precision (Class 1): 0.04\n",
    "Recall (Class 1): 0.25\n",
    "F1-score (Class 1): N/A (Due to zero true positives)\n",
    "Accuracy: 0.77\n",
    "Randomized Logistic Regression:\n",
    "\n",
    "Precision (Class 1): 0.05\n",
    "Recall (Class 1): 0.55\n",
    "F1-score (Class 1): 0.09\n",
    "Accuracy: N/A\n",
    "Support Vector Machine (SVM):\n",
    "\n",
    "Precision (Class 1): 0.05\n",
    "Recall (Class 1): 0.49\n",
    "F1-score (Class 1): 0.09\n",
    "Accuracy: 0.66\n",
    "Gradient Boosting:\n",
    "\n",
    "Precision (Class 1): 0.05\n",
    "Recall (Class 1): 0.49\n",
    "F1-score (Class 1): 0.09\n",
    "Accuracy: 0.92\n",
    "Decision Tree:\n",
    "\n",
    "Precision (Class 1): 0.05\n",
    "Recall (Class 1): 0.47\n",
    "F1-score (Class 1): 0.09\n",
    "Accuracy: 0.91\n",
    "Bagging (Ensemble of Decision Trees):\n",
    "\n",
    "Test Set Score: Varies based on the run\n",
    "AdaBoost (Ensemble of Decision Trees):\n",
    "\n",
    "Test Set Score: Varies based on the run\n",
    "Voting Classifier (Ensemble of Logistic Regression, K-Nearest Neighbors, Gradient Boosting):\n",
    "\n",
    "Test Set Score: Varies based on the run\n",
    "\n",
    "## Suggested Model for Production:\n",
    "Gradient Boosting: This model consistently shows good performance across precision, recall, and F1-score for the positive class. Additionally, it has a high accuracy on the test set, indicating overall good predictive capability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
